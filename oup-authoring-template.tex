%%
%% Copyright 2022 OXFORD UNIVERSITY PRESS
%%
%% This file is part of the 'oup-authoring-template Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'oup-authoring-template Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for OXFORD UNIVERSITY PRESS's document class `oup-authoring-template'
%% with bibliographic references
%%

%%%CONTEMPORARY%%%
\documentclass[unnumsec,webpdf,contemporary,large]{oup-authoring-template}%
%\documentclass[unnumsec,webpdf,contemporary,large,namedate]{oup-authoring-template}% uncomment this line for author year citations and comment the above
%\documentclass[unnumsec,webpdf,contemporary,medium]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,contemporary,small]{oup-authoring-template}

%%%MODERN%%%
%\documentclass[unnumsec,webpdf,modern,large]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,modern,large,namedate]{oup-authoring-template}% uncomment this line for author year citations and comment the above
%\documentclass[unnumsec,webpdf,modern,medium]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,modern,small]{oup-authoring-template}

%%%TRADITIONAL%%%
%\documentclass[unnumsec,webpdf,traditional,large]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,traditional,large,namedate]{oup-authoring-template}% uncomment this line for author year citations and comment the above
%\documentclass[unnumsec,namedate,webpdf,traditional,medium]{oup-authoring-template}
%\documentclass[namedate,webpdf,traditional,small]{oup-authoring-template}

%\onecolumn % for one column layouts

%\usepackage{showframe}

\graphicspath{{Fig/}}

% line numbers
%\usepackage[mathlines, switch]{lineno}
%\usepackage[right]{lineno}

\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}%
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.
\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%
\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}

\begin{document}

\journaltitle{Journal Title Here}
\DOI{DOI HERE}
\copyrightyear{2022}
\pubyear{2019}
\access{Advance Access Publication Date: Day Month Year}
\appnotes{Paper}

\firstpage{1}

%\subtitle{Subject Section}

\title[nf-core/viralgenie: A Novel Pipeline For Untargeted Viral Genome Reconstruction]{nf-core/viralgenie: A Novel Pipeline For Untargeted Viral Genome Reconstruction}

\author[1,$\ast$]{Joon Klaps\ORCID{0000-0002-2507-0430}}
\author[1]{Philippe Lemey\ORCID{0000-0003-2826-5353}}
\author[1]{Liana Kafetzopoulou\ORCID{0000-0003-4531-1374}}

\authormark{Joon Klaps et al.}

\address[1]{\orgdiv{Rega Institute for Medical Research Department of Microbiology, Immunology and Transplantation Department of Pharmaceutical and Pharmacological Sciences
}, \orgname{KU Leuven}, \orgaddress{\street{Herestraat 49}, \postcode{3000}, \state{Leuven}, \country{Belgium}}}
% \address[2]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \postcode{Postcode}, \state{State}, \country{Country}}}
% \address[3]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \postcode{Postcode}, \state{State}, \country{Country}}}
% \address[4]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \postcode{Postcode}, \state{State}, \country{Country}}}

\corresp[$\ast$]{Corresponding author. \href{email:joon.klaps@kuleuven.be}{joon.klaps@kuleuven.be}}

\received{Date}{0}{Year}
\revised{Date}{0}{Year}
\accepted{Date}{0}{Year}

%\editor{Associate Editor: Name}

\abstract{
\textbf{Motivation:} Eukaryotic viruses present significant challenges in genome reconstruction and variant analysis due to their extensive diversity, quasi-species, absence of universal marker genes, and genome segmentation. While de novo assembly followed by reference database matching and consequently, read mapping is a common approach, manual execution of this workflow is extremely time-consuming, particularly due to the extensive reference verification and selection required. There is a critical need for an automated, scalable pipeline that can efficiently handle viral metagenomic analysis without manual intervention.\\
\textbf{Results:} Here, we present nf-core/viralgenie, a comprehensive viral metagenomic pipeline for untargeted genome reconstruction, and variant analysis of eukaryotic viruses. Viralgenie is implemented as a modular Nextflow workflow that processes metagenomic and hybridization capture enriched samples to automatically detect and assemble viral genomes, while also performing variant analysis. The pipeline features automated reference selection, quality control metrics, comprehensive documentation, and seamless integration with containerization technologies including Docker, Singularity, and Podman. We demonstrate its utility and accuracy through validation on both simulated and real datasets, showing robust performance across diverse viral families and sample types.\\
\textbf{Availability:} nf-core/viralgenie is freely available at https://github.com/nf-core/viralgenie with comprehensive documentation at https://nf-co.re/viralgenie.\\
% \textbf{Contact:} \href{joon.klaps@kuleuven.be}{joon.klaps@kuleuven.be}\\
% \textbf{Supplementary information:} Supplementary data are available at \textit{Bioinformatics} online.
}
\keywords{viralgenie, bioinformatic pipeline, nextflow, viral metagenomics, viral assembly, viral variant analysis}

% \boxedtext{
% \begin{itemize}
% \item Key boxed text here.
% \item Key boxed text here.
% \item Key boxed text here.
% \end{itemize}}

\maketitle


\section{Introduction}\label{sec1}

Reconstructing viral genomes from metagenomic sequencing data presents significant computational challenges, particularly for eukaryotic viruses that exhibit extensive genetic diversity, and quasi-species formation. This diversity is further compounded by the prevalence of segmented genomes in many viral families, including influenza, rotavirus, and bunyaviruses, where individual segments may undergo independent evolutionary pressures and reassortment events.

Resulting in a challenging landscape for viral genome reconstruction. Typically, for accurate and complete viral genome reconstruction, manual curation of contigs and reference matching is required. This process is not only time-consuming making it impractical for large-scale studies or rapid response scenarios such as emerging viral outbreaks of unknown origin. The need for a more automated and scalable solution has become increasingly apparent, particularly in the context of public health surveillance and epidemiological research [NEED FOR CITATION].

% Current bioinformatics solutions in the nf-core ecosystem address specific aspects of viral analysis but lack comprehensive coverage for untargeted viral genome reconstruction. The nf-core/viralrecon pipeline excels in targeted analysis scenarios where reference genomes are predetermined, making it well-suited for surveillance of known pathogens such as SARS-CoV-2. However, this targeted approach becomes limiting when analyzing environmental samples, investigating novel viral outbreaks, or studying highly divergent viral strains where the appropriate reference genome is unknown a priori. Similarly, nf-core/mag focuses on comprehensive metagenomic analysis but is optimized for describing entire microbial communities rather than specifically targeting viral diversity.

% The gap between targeted and metagenomic approaches becomes particularly pronounced when studying intrahost viral evolution, viral co-infections, or reassortment events in segmented viruses. These research questions require the detection and reconstruction of multiple related viral genomes from the same sample, a capability that existing pipelines struggle to provide efficiently. Furthermore, the lack of standardized workflows for untargeted viral analysis hinders reproducibility and limits the adoption of best practices across the viral genomics community.

To address these limitations, we developed nf-core/viralgenie, a comprehensive pipeline specifically designed for untargeted viral genome reconstruction. The pipeline implements an automated workflow that performs de novo assembly, reference matching through sequence clustering, and iterative refinement by read mapping and consensus calling to reconstruct viral genomes without prior knowledge of the target sequences. By integrating containerization technologies and following nf-core standards, viralgenie ensures reproducibility and scalability across diverse computational environments while maintaining the flexibility required for varied research applications.

\section{Methods}\label{sec2}

The nf-core/viralgenie pipeline implements a comprehensive workflow for untargeted viral genome reconstruction and variant analysis, consisting of five major analytical subworkflows: preprocessing, metagenomic diversity assessment, assembly and polishing, variant analysis with iterative refinement, and consensus quality control. The pipeline is implemented in Nextflow and follows nf-core standards \cite{Krakau2022-qh}, ensuring reproducibility and portability across computational environments through containerization with Docker, Singularity, or Conda.

\subsection{Pipeline Overview and Installation}\label{subsec_overview}

Viralgenie requires Nextflow and a container management system (Docker, Singularity, or Conda). The pipeline can be executed with minimal setup:

\begin{verbatim}
nextflow run nf-core/viralgenie \
    -profile docker \
    --input samplesheet.csv
\end{verbatim}

Input data is provided through a samplesheet in CSV, TSV, YAML, or JSON format containing sample names and paths to FASTQ files. The pipeline supports both single-end and paired-end sequencing data, with optional support for Unique Molecular Identifiers (UMIs) and mapping constraints for reference-guided analysis.

\subsection{Read Preprocessing}\label{subsec_preprocessing}

The preprocessing module performs quality control and filtering of raw sequencing reads through five sequential steps. Initial quality assessment is conducted using FastQC before and after each processing step to monitor data quality throughout the workflow.

Adapter trimming and read processing is performed using either fastp (default) or Trimmomatic, both of which provide comprehensive adapter removal and quality filtering capabilities. For libraries prepared with UMIs, PCR duplicate removal is implemented using HUMID, which supports both directional and maximum clustering methods for UMI-based deduplication. The directional method (default) accounts for expected PCR errors by grouping reads using the relationship: node A counts $\geq$ (2 $\times$ node B counts) - 1.

Read merging is performed when multiple sequencing runs exist for the same sample, concatenating R1 files with R1 and R2 files with R2, while maintaining separation between single-end and paired-end data. Complexity filtering, implemented through BBduk or prinseq++, removes low-complexity sequences containing repetitive elements that could produce spurious alignments during downstream analysis.

Host contamination removal is performed using Kraken2 \cite{Wood2019-jl} against a user-specified host genome database. The default database contains a subset of the human genome, though users are strongly encouraged to employ more comprehensive databases including complete host genomes, common sequencer contaminants, and bacterial genomes to ensure thorough decontamination.

\subsection{Metagenomic Diversity Assessment}\label{subsec_diversity}

Taxonomic classification of processed reads is performed using two complementary approaches to maximize detection sensitivity across diverse viral families. Kaiju \cite{Menzel2016-tz} performs protein-based classification using a Burrows-Wheeler transform search strategy against annotated protein-coding genes from microbial genomes, enabling detection of highly divergent sequences through amino acid conservation. Kraken2 \cite{Wood2019-jl} provides DNA-level classification using k-mer mapping to identify the lowest common ancestor (LCA) of genomes containing specific k-mers. Optional Bracken analysis can be enabled for abundance estimation, though viral abundance comparisons should be interpreted cautiously due to the absence of universal marker genes in viruses.

Results from both classifiers are visualized using Krona, which generates interactive multi-layered pie charts allowing hierarchical exploration of taxonomic diversity. This dual-classification approach compensates for the limitations of individual methods and provides comprehensive coverage of viral diversity in metagenomic samples.

\subsection{Assembly and Polishing}\label{subsec_assembly}

The assembly module implements a multi-assembler approach followed by sophisticated clustering and scaffolding procedures. De novo assembly is performed using three complementary assemblers: SPAdes \cite{Bankevich2012-lh} (configured for RNA viral mode by default \cite{Meleshko2021-gb}), MEGAHIT, and Trinity. This multi-assembler strategy capitalizes on the distinct algorithmic strengths of each tool to maximize genome recovery across diverse viral families and coverage distributions.

Assembled contigs undergo extension using SSPACE Basic, which leverages paired-end read information to scaffold and extend initial assemblies. Coverage calculation is performed by mapping processed reads back to contigs using BWAmem2 (default), BWA, or Bowtie2, enabling identification and filtration of low-coverage assemblies that likely represent assembly artifacts.

Reference matching is conducted through BLASTn searches against a comprehensive reference sequence pool, with the default being the latest clustered Reference Viral Database (RVDB) \cite{Goodacre2018-dw}. The top five BLAST hits for each contig are retained and incorporated into subsequent clustering analysis, facilitating identification of related genomic segments and appropriate reference sequences for scaffolding.

Taxonomy-guided clustering employs a two-stage process to group related contigs. Initial pre-clustering uses taxonomic assignments from both Kraken2 and Kaiju to resolve classification inconsistencies and separate contigs by taxonomic identity. Subsequent nucleotide similarity clustering is performed using one of six available algorithms: CD-HIT-EST \cite{Li2006-nj}, VSEARCH \cite{Rognes2016-ju}, MMseqs-linclust, MMseqs-cluster, vRhyme, or Mash with network-based community detection. The choice of clustering method allows optimization for specific dataset characteristics, with CD-HIT-EST providing speed for smaller datasets and MMseqs variants offering scalability for larger analyses.

Final scaffolding maps all cluster members to their respective centroids using Minimap2 \cite{Li2018-gi}, followed by consensus calling with iVar to generate reference-assisted assemblies. Regions with zero coverage depth are optionally annotated using reference sequences to produce complete genome reconstructions.

\subsection{Variant Analysis and Iterative Refinement}\label{subsec_variant}

The variant calling module supports two distinct analytical pathways: external reference-based analysis and de novo assembly refinement. In external reference-based analysis, users provide reference genomes through mapping constraints, with automatic selection of the most appropriate references using Mash \cite{Ondov2019-bo} k-mer distance calculations. This approach selects references sharing the highest number of k-mers with the sequencing reads, minimizing mapping bias for highly divergent viral sequences.

For de novo assembly refinement, the pipeline performs iterative improvement of initially assembled consensus genomes. Each iteration maps reads back to the current consensus using BWAmem2, BWA, or Bowtie2, followed by variant calling and consensus generation. The default configuration performs two refinement iterations, though this is user-configurable.

Variant calling is implemented using either BCFtools or iVar, each offering distinct advantages for viral genomics applications. BCFtools provides higher precision through sophisticated statistical modeling but may miss low-frequency variants. iVar excels at detecting multiallelic sites and low-frequency variants, making it particularly suitable for viral quasi-species analysis. iVar also handles ambiguous nucleotides more effectively, representing multiallelic positions with IUPAC ambiguity codes rather than masking them.

Optional UMI-based deduplication can be performed using UMI-tools, while standard PCR duplicate removal utilizes Picard MarkDuplicates. Comprehensive mapping statistics are generated using samtools (flagstat, idxstats, stats), Picard CollectMultipleMetrics, and mosdepth for coverage analysis.

Variant filtering removes variants with insufficient depth or quality, with BCFtools implementing additional steps to handle multiallelic sites and merge SNPs with indels. The final consensus sequences incorporate high-quality variants while maintaining genomic completeness through reference-guided gap filling.

\subsection{Consensus Quality Control and Annotation}\label{subsec_qc}

Comprehensive quality assessment of reconstructed viral genomes is performed through multiple complementary analyses. QUAST provides standard assembly metrics including contig statistics, N50 values, and quantification of ambiguous bases, which serves as a primary indicator of consensus quality. CheckV \cite{Nayfach2021-wl} estimates genome completeness and contamination by comparison against a curated database of complete viral genomes, though completeness estimates for segmented viruses should be interpreted considering that CheckV calculates completeness based on concatenated segment lengths.

Functional annotation is performed using Prokka, which identifies coding sequences and assigns functional annotations. While originally designed for bacterial genomes, Prokka provides reasonable annotation for viral sequences, particularly when supplemented with custom viral protein databases such as prot-RVDB.

Consensus genomes undergo similarity analysis through BLASTn searches against the reference pool and MMseqs searches against comprehensive annotation databases such as Virosaurus \cite{Gleizes2020-rq}. MMseqs enables rapid tblastx-equivalent searches for highly divergent sequences while maintaining nucleotide database compatibility. The annotation pipeline extracts species identification, segment designation, expected host information, and additional metadata from the best database matches.

Multiple sequence alignment using MAFFT aligns final consensus genomes with their corresponding references and constituent de novo contigs, enabling assessment of assembly accuracy and identification of genomic variations. Variant functional annotation is performed using SnpEff, which predicts the biological impact of detected variants, including synonymous/non-synonymous classifications and amino acid changes.

All quality control metrics are integrated into interactive MultiQC reports, providing comprehensive visualization of pipeline results. Custom summary tables extract key metrics from each analysis tool, facilitating rapid assessment of reconstruction quality across multiple samples.

\section{Results}\label{sec3}

The nf-core/viralgenie pipeline provides comprehensive outputs enabling thorough evaluation of viral genome reconstruction quality and downstream analysis preparation. This section describes the expected outputs, runtime characteristics, and parameter selection rationale that guide optimal pipeline performance.

\subsection{Pipeline Performance and Runtime Metrics}\label{subsec_performance}

Viralgenie demonstrates efficient computational performance across diverse sample types and scales. Runtime scales primarily with read depth and viral diversity, with typical processing times ranging from 2-6 hours for standard metagenomic samples (10-50 million reads) on modern compute clusters. The multi-assembler approach adds computational overhead compared to single-assembler pipelines but provides superior genome recovery, particularly for highly divergent or low-coverage viral sequences.

Memory requirements vary by analysis module, with assembly typically representing the most resource-intensive step. SPAdes requires the highest memory allocation (8-32 GB depending on dataset size), while MEGAHIT and Trinity offer more memory-efficient alternatives. The clustering and variant calling steps scale efficiently with read depth, maintaining reasonable resource requirements even for high-coverage datasets.

Pipeline scalability benefits from Nextflow's built-in parallelization capabilities, enabling concurrent processing of multiple samples and assembly methods. Resource allocation can be customized through configuration profiles, allowing optimization for different computational environments from local workstations to high-performance computing clusters.

\subsection{Output Organization and Interpretation}\label{subsec_output}

Viralgenie generates a hierarchical output structure designed for intuitive navigation and comprehensive result interpretation. The primary MultiQC report serves as the central hub for quality assessment, presenting interactive visualizations of all major pipeline metrics. Custom summary tables within the MultiQC report extract key information from each analysis tool, enabling rapid identification of high-quality consensus genomes and potential issues requiring attention.

Consensus sequences are organized by sample and clustering results, with clear naming conventions indicating assembly methods and refinement iterations. Each consensus genome is accompanied by comprehensive metadata including quality metrics, annotation results, and mapping statistics. Intermediate files are preserved to enable detailed troubleshooting and alternative parameter exploration.

The \texttt{overview-tables} directory contains summarized results from all major analysis steps, providing convenient access to quantitative metrics for downstream analysis or publication. These tables include assembly statistics, taxonomy assignments, variant calling results, and quality control metrics in standardized formats compatible with common statistical software packages.

\subsection{Default Parameter Selection and Tool Choices}\label{subsec_parameters}

The pipeline's default parameters reflect extensive benchmarking and optimization for viral metagenomic applications. Tool selection balances computational efficiency with analytical sensitivity, prioritizing methods that perform well across diverse viral families and sample types.

For clustering applications, the default CD-HIT-EST \cite{Li2006-nj} algorithm with 85\% similarity threshold provides an optimal balance between sensitivity and specificity for most viral datasets. This threshold effectively groups related genomic segments while maintaining separation of distinct viral strains. Alternative clustering methods are provided to accommodate specific research needs: VSEARCH \cite{Rognes2016-ju} for enhanced accuracy, MMseqs variants for scalability, and Mash \cite{Ondov2019-bo} for rapid approximate clustering.

Variant calling defaults favor iVar over BCFtools for consensus generation due to its superior handling of viral-specific challenges including multiallelic sites, low-frequency variants, and ambiguous base calling. However, BCFtools is employed for intermediate refinement steps where its conservative approach helps prevent error propagation during iterative improvement.

Database selections prioritize comprehensive coverage while maintaining computational tractability. The clustered RVDB serves as the default reference pool, providing broad viral representation while limiting computational requirements. For taxonomic classification, viral-specific databases are employed to maximize detection sensitivity for eukaryotic viruses while minimizing false positive assignments from bacterial or archaeal sequences.

Quality thresholds are conservatively set to ensure high-confidence results while accommodating the inherent challenges of viral genome reconstruction. Minimum read depth requirements (10x for variant calling), quality scores (Phred 20), and coverage thresholds are calibrated based on empirical performance across diverse viral families and sample preparation methods.

\section{Discussion}\label{sec4}

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

\paragraph{This is an example for fourth level head - paragraph head}

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.


%%%%%%%%%%%%%%

\section{Competing interests}
No competing interest is declared.

\section{Author contributions statement}

Must include all authors, identified by initials, for example:
S.R. and D.A. conceived the experiment(s),  S.R. conducted the experiment(s), S.R. and D.A. analysed the results.  S.R. and D.A. wrote and reviewed the manuscript.

\section{Acknowledgments}
The authors thank the anonymous reviewers for their valuable suggestions. This work is supported in part by funds from the National Science Foundation (NSF: \# 1636933 and \# 1920920).


\bibliographystyle{plain}
\bibliography{reference}

%USE THE BELOW OPTIONS IN CASE YOU NEED AUTHOR YEAR FORMAT.
%\bibliographystyle{abbrvnat}
%\bibliography{reference}



%% sample for biography with author's image
% \begin{biography}{{\color{black!20}\rule{77pt}{77pt}}}{\author{Author Name.} This is sample author biography text. The values provided in the optional argument are meant for sample purposes. There is no need to include the width and height of an image in the optional argument for live articles. This is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text.}
% \end{biography}

%% sample for biography without author's image
% \begin{biography}{}{\author{Author Name.} This is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text.}
% \end{biography}

\end{document}
